{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eH5F_9OpIrD5"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchaudio onnx onnxruntime nemo_toolkit[all]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import nemo.collections.asr as nemo_asr\n",
        "\n",
        "# üîπ Carregar modelo pr√©-treinado\n",
        "asr_model = nemo_asr.models.EncDecCTCModel.from_pretrained(\n",
        "    model_name=\"neongeckocom/stt_pt_citrinet_512_gamma_0_25\"\n",
        ")\n",
        "\n",
        "# üîπ Combinar encoder + decoder manualmente\n",
        "class CitrinetCore(torch.nn.Module):\n",
        "    def __init__(self, nemo_model):\n",
        "        super().__init__()\n",
        "        self.encoder = nemo_model.encoder\n",
        "        self.decoder = nemo_model.decoder\n",
        "\n",
        "    def forward(self, features, features_len):\n",
        "        encoded, encoded_len = self.encoder(audio_signal=features, length=features_len)\n",
        "        logits = self.decoder(encoder_output=encoded)\n",
        "        return logits, encoded_len\n",
        "\n",
        "core_model = CitrinetCore(asr_model)\n",
        "core_model.eval()\n",
        "\n",
        "# üîπ Dummy input (como o featurizer produziria)\n",
        "dummy_feat = torch.randn(1, 80, 200, dtype=torch.float32)   # [B, mel_bins, frames]\n",
        "dummy_len = torch.tensor([200], dtype=torch.int64)\n",
        "\n",
        "# üîπ Exportar para ONNX\n",
        "torch.onnx.export(\n",
        "    core_model,\n",
        "    (dummy_feat, dummy_len),\n",
        "    \"citrinet_encoder_decoder.onnx\",\n",
        "    input_names=[\"features\", \"features_len\"],\n",
        "    output_names=[\"logits\", \"encoded_len\"],\n",
        "    dynamic_axes={\n",
        "        \"features\": {2: \"n_frames\"},\n",
        "        \"logits\": {1: \"n_frames\"},\n",
        "    },\n",
        "    opset_version=14,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Exportado: citrinet_encoder_decoder.onnx\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NmfA5WwKXK6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extrair tokens do modelo NeMo\n",
        "tokens = asr_model.decoder.vocabulary  # lista de strings, cada token\n",
        "\n",
        "# Salvar em tokens.txt\n",
        "with open(\"tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for token in tokens:\n",
        "        f.write(token + \"\\n\")\n",
        "\n",
        "print(\"‚úÖ tokens.txt criado com sucesso!\")"
      ],
      "metadata": {
        "id": "CkYiOuX_H9Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from omegaconf import OmegaConf\n",
        "import yaml\n",
        "\n",
        "# Extrair a estrutura como dicion√°rio nativo do Python\n",
        "config_dict = OmegaConf.to_container(asr_model.cfg, resolve=True)\n",
        "\n",
        "# Salvar como YAML\n",
        "with open(\"model_config.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
        "    yaml.dump(config_dict, f, allow_unicode=True)\n",
        "\n",
        "print(\"‚úÖ Arquivo 'model_config.yaml' exportado com sucesso!\")\n"
      ],
      "metadata": {
        "id": "pQAcc1wT1bDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "# Caminho para o modelo exportado\n",
        "onnx_path = \"citrinet_encoder_decoder.onnx\"\n",
        "\n",
        "# 1Ô∏è‚É£ Carregar o modelo ONNX\n",
        "model = onnx.load(onnx_path)\n",
        "\n",
        "print(\"=== ENTRADAS DO MODELO ===\")\n",
        "for inp in model.graph.input:\n",
        "    dims = [d.dim_value for d in inp.type.tensor_type.shape.dim]\n",
        "    dtype = inp.type.tensor_type.elem_type\n",
        "    print(f\"- {inp.name} | shape: {dims} | dtype: {dtype}\")\n",
        "\n",
        "print(\"\\n=== SA√çDAS DO MODELO ===\")\n",
        "for out in model.graph.output:\n",
        "    dims = [d.dim_value for d in out.type.tensor_type.shape.dim]\n",
        "    dtype = out.type.tensor_type.elem_type\n",
        "    print(f\"- {out.name} | shape: {dims} | dtype: {dtype}\")\n",
        "\n",
        "# 2Ô∏è‚É£ Verificar se o modelo cont√©m o decoder\n",
        "# Dica: modelos com decoder (CTC) costumam ter \"logits\" ou \"probs\" na sa√≠da\n",
        "output_names = [out.name.lower() for out in model.graph.output]\n",
        "if any(\"logits\" in n or \"probs\" in n or \"ctc\" in n for n in output_names):\n",
        "    print(\"\\n‚úÖ Este modelo cont√©m o DECODER (CTC head).\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Este modelo provavelmente cont√©m apenas o ENCODER.\")\n",
        "\n",
        "# 3Ô∏è‚É£ Pr√©-processamento esperado\n",
        "\n",
        "def preprocess_audio(filepath, sample_rate=16000):\n",
        "    # Carregar √°udio\n",
        "    waveform, sr = torchaudio.load(filepath)\n",
        "    if sr != sample_rate:\n",
        "        waveform = torchaudio.functional.resample(waveform, sr, sample_rate)\n",
        "\n",
        "    # Converter para mono\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    # Normalizar (amplitude entre -1 e 1)\n",
        "    waveform = waveform / torch.abs(waveform).max()\n",
        "\n",
        "    # Gerar espectrograma mel\n",
        "    mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=sample_rate,\n",
        "        n_fft=512,\n",
        "        win_length=400,\n",
        "        hop_length=160,\n",
        "        n_mels=80\n",
        "    )(waveform)\n",
        "\n",
        "    # Converter para log-mel\n",
        "    log_mel_spec = torch.log(mel_spec + 1e-6)\n",
        "\n",
        "    print(\"\\n=== Pr√©-processamento conclu√≠do ===\")\n",
        "    print(f\"Waveform shape: {waveform.shape}\")\n",
        "    print(f\"Log-mel shape: {log_mel_spec.shape}\")\n",
        "    print(f\"Exemplo de entrada esperado: (batch=1, n_mels=80, time={log_mel_spec.shape[-1]})\")\n",
        "\n",
        "    return log_mel_spec\n",
        "\n",
        "# Exemplo de uso:\n",
        "# log_mel = preprocess_audio(\"teste.wav\")\n"
      ],
      "metadata": {
        "id": "Eaf4WNfA2QqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "\n",
        "session = ort.InferenceSession(\"citrinet_encoder_decoder.onnx\", providers=[\"CPUExecutionProvider\"])\n",
        "\n",
        "input_name = session.get_inputs()[0].name\n",
        "output_name = session.get_outputs()[0].name\n",
        "\n",
        "print(\"Input name:\", input_name)\n",
        "print(\"Output name:\", output_name)\n"
      ],
      "metadata": {
        "id": "FpsJanrG52tz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}